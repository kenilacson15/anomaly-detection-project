{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "### Step 1: Import Dependencies\n",
    "This cell imports all required libraries for:\n",
    "- Data manipulation (`pandas`, `numpy`)\n",
    "- Feature scaling (`StandardScaler` from `sklearn.preprocessing`)\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "### Step 2: Define File Paths\n",
    "- `input_file`: Path to the processed dataset from the preprocessing step.\n",
    "- `output_dir`: Directory where the feature-engineered dataset will be saved.\n",
    "- `output_file`: Full path to the output CSV file.\n",
    "\"\"\"\n",
    "\n",
    "input_file = r\"C:\\Users\\Ken Ira Talingting\\Desktop\\anomaly-detection-project\\data\\processed\\equipment_anomaly_data_processed.csv\"\n",
    "output_dir = r\"C:\\Users\\Ken Ira Talingting\\Desktop\\anomaly-detection-project\\data\\processed\"\n",
    "output_file = os.path.join(output_dir, \"equipment_anomaly_data_feature_engineered.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "### Step 3: Load the Processed Dataset\n",
    "- Reads the preprocessed dataset from CSV.\n",
    "- Displays basic information about the dataset, such as shape and sample records.\n",
    "\"\"\"\n",
    "\n",
    "df = pd.read_csv(input_file)\n",
    "\n",
    "print(f\"‚úÖ Dataset loaded successfully: {df.shape[0]} rows, {df.shape[1]} columns.\")\n",
    "display(df.head())  # Using display() for better visualization in Jupyter Notebook\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "### Step 4: Check for Missing Values\n",
    "- Identifies any missing values in the dataset.\n",
    "- If missing values are found, they should be handled accordingly.\n",
    "\"\"\"\n",
    "\n",
    "missing_values = df.isnull().sum()\n",
    "print(\"üîç Missing values per column:\\n\", missing_values)\n",
    "\n",
    "# Uncomment below if you want to handle missing values (example)\n",
    "# df.fillna(method='ffill', inplace=True)  # Forward fill to propagate last valid observation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "### Step 5: Feature Engineering\n",
    "This step creates new features to enhance the dataset:\n",
    "\n",
    "1. **Ratio Features**:\n",
    "   - `temp_pressure_ratio` = Temperature / Pressure\n",
    "   - `vib_humidity_ratio` = Vibration / Humidity\n",
    "\n",
    "2. **Interaction Features**:\n",
    "   - `pressure_vibration_prod` = Pressure * Vibration\n",
    "   - `temp_humidity_prod` = Temperature * Humidity\n",
    "\n",
    "3. **Logarithmic Transformation**:\n",
    "   - `log_vibration` = Log-transformed Vibration (to reduce skewness)\n",
    "\n",
    "A small constant (`epsilon`) is added to avoid division by zero issues.\n",
    "\"\"\"\n",
    "\n",
    "epsilon = 1e-6  # Small constant to prevent division by zero\n",
    "\n",
    "# Create ratio features\n",
    "df['temp_pressure_ratio'] = df['temperature'] / (df['pressure'] + epsilon)\n",
    "df['vib_humidity_ratio'] = df['vibration'] / (df['humidity'] + epsilon)\n",
    "\n",
    "# Create interaction features\n",
    "df['pressure_vibration_prod'] = df['pressure'] * df['vibration']\n",
    "df['temp_humidity_prod'] = df['temperature'] * df['humidity']\n",
    "\n",
    "# Log transformation for vibration (useful for skewed distributions)\n",
    "df['log_vibration'] = np.log(df['vibration'] + epsilon)\n",
    "\n",
    "print(\"‚úÖ Feature engineering completed. Sample of new features:\")\n",
    "display(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "### Step 6: Categorical Encoding\n",
    "- Converts categorical variables into numerical format using **one-hot encoding**.\n",
    "- Drops the first category to avoid multicollinearity (`drop_first=True`).\n",
    "\n",
    "Categorical columns:\n",
    "- `equipment`\n",
    "- `location`\n",
    "\"\"\"\n",
    "\n",
    "df_encoded = pd.get_dummies(df, columns=['equipment', 'location'], drop_first=True)\n",
    "\n",
    "print(f\"‚úÖ Categorical encoding completed. New dataset shape: {df_encoded.shape}\")\n",
    "display(df_encoded.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "### Step 7: Scale Numerical Features\n",
    "- Standardizes numerical features using `StandardScaler` to improve model performance.\n",
    "- Ensures that all features have **zero mean** and **unit variance**.\n",
    "\n",
    "Columns being scaled:\n",
    "- Original sensor readings (`temperature`, `pressure`, `vibration`, `humidity`)\n",
    "- Newly engineered features (`temp_pressure_ratio`, `vib_humidity_ratio`, etc.)\n",
    "\"\"\"\n",
    "\n",
    "numeric_cols = [\n",
    "    'temperature', 'pressure', 'vibration', 'humidity',\n",
    "    'temp_pressure_ratio', 'vib_humidity_ratio',\n",
    "    'pressure_vibration_prod', 'temp_humidity_prod', 'log_vibration'\n",
    "]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "df_encoded[numeric_cols] = scaler.fit_transform(df_encoded[numeric_cols])\n",
    "\n",
    "print(\"‚úÖ Feature scaling completed. Sample of scaled data:\")\n",
    "display(df_encoded.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "### Step 8: Save the Feature-Engineered Dataset\n",
    "- Saves the processed dataset to CSV for further analysis or model training.\n",
    "\"\"\"\n",
    "\n",
    "df_encoded.to_csv(output_file, index=False)\n",
    "print(f\"‚úÖ Feature engineered dataset saved to: {output_file}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
