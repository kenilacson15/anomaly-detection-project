{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CONFIGURATION & PATHS\n",
    "# =============================================================================\n",
    "\n",
    "import os\n",
    "import json\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# File paths (adjust these paths as needed)\n",
    "INPUT_FILE = r\"C:\\Users\\Ken Ira Talingting\\Desktop\\anomaly-detection-project\\data\\processed\\equipment_anomaly_data_feature_engineered.csv\"\n",
    "OUTPUT_DIR = Path(r\"C:\\Users\\Ken Ira Talingting\\Desktop\\anomaly-detection-project\\data\\processed_results\")\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)  # Ensure the output directory exists\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "tf.random.set_seed(RANDOM_STATE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# DATA LOADING & PREPROCESSING FUNCTIONS\n",
    "# =============================================================================\n",
    "\n",
    "def load_data(file_path):\n",
    "    \"\"\"\n",
    "    Load the CSV file into a pandas DataFrame.\n",
    "    \n",
    "    Parameters:\n",
    "        file_path (str): Path to the CSV file.\n",
    "        \n",
    "    Returns:\n",
    "        df (DataFrame): Loaded DataFrame.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(file_path)\n",
    "    # Convert boolean columns to integer (if any)\n",
    "    bool_cols = df.select_dtypes(include='bool').columns\n",
    "    if len(bool_cols) > 0:\n",
    "        df[bool_cols] = df[bool_cols].astype(int)\n",
    "    return df\n",
    "\n",
    "def preprocess_data(df, target_column='faulty'):\n",
    "    \"\"\"\n",
    "    Separate features and target, and apply scaling.\n",
    "    \n",
    "    Parameters:\n",
    "        df (DataFrame): Input DataFrame.\n",
    "        target_column (str): Name of the column containing ground truth anomaly labels.\n",
    "        \n",
    "    Returns:\n",
    "        X_scaled (DataFrame): Scaled feature data for autoencoder training.\n",
    "        y (np.array): Ground truth anomaly labels.\n",
    "        scaler (StandardScaler): Fitted scaler (for inverse transforming or future use).\n",
    "    \"\"\"\n",
    "    # Separate features and target variable\n",
    "    X = df.drop(columns=[target_column])\n",
    "    y = df[target_column].values\n",
    "\n",
    "    # Scale features to zero mean and unit variance.\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = pd.DataFrame(scaler.fit_transform(X), columns=X.columns)\n",
    "    \n",
    "    return X_scaled, y, scaler\n",
    "\n",
    "# Example usage:\n",
    "# df = load_data(INPUT_FILE)\n",
    "# X, y, scaler = preprocess_data(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# AUTOENCODER MODEL DEFINITION\n",
    "# =============================================================================\n",
    "\n",
    "def build_autoencoder(input_dim, encoding_dim=8):\n",
    "    \"\"\"\n",
    "    Build a simple fully-connected autoencoder using Keras.\n",
    "    \n",
    "    Parameters:\n",
    "        input_dim (int): Number of input features.\n",
    "        encoding_dim (int): Dimension of the latent (encoded) space.\n",
    "        \n",
    "    Returns:\n",
    "        autoencoder (Model): Compiled autoencoder model.\n",
    "    \"\"\"\n",
    "    # Input layer\n",
    "    input_layer = keras.Input(shape=(input_dim,))\n",
    "    \n",
    "    # Encoder: compress input to latent representation\n",
    "    encoded = layers.Dense(encoding_dim * 2, activation='relu')(input_layer)\n",
    "    encoded = layers.Dense(encoding_dim, activation='relu')(encoded)\n",
    "    \n",
    "    # Decoder: reconstruct input from latent space\n",
    "    decoded = layers.Dense(encoding_dim * 2, activation='relu')(encoded)\n",
    "    decoded = layers.Dense(input_dim, activation='linear')(decoded)\n",
    "    \n",
    "    # Build and compile the autoencoder model\n",
    "    autoencoder = keras.Model(inputs=input_layer, outputs=decoded)\n",
    "    autoencoder.compile(optimizer='adam', loss='mse')\n",
    "    \n",
    "    return autoencoder\n",
    "\n",
    "# Example usage:\n",
    "# autoencoder = build_autoencoder(input_dim=X.shape[1], encoding_dim=8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# TRAINING & EVALUATION FUNCTIONS\n",
    "# =============================================================================\n",
    "\n",
    "def train_autoencoder(model, X, epochs=50, batch_size=32):\n",
    "    \"\"\"\n",
    "    Train the autoencoder model.\n",
    "    \n",
    "    Parameters:\n",
    "        model (Model): Compiled Keras autoencoder.\n",
    "        X (DataFrame): Training data (features).\n",
    "        epochs (int): Number of training epochs.\n",
    "        batch_size (int): Batch size for training.\n",
    "        \n",
    "    Returns:\n",
    "        history: Training history (for plotting losses).\n",
    "    \"\"\"\n",
    "    history = model.fit(\n",
    "        X, X,  # Input and target are the same for an autoencoder\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        validation_split=0.1,  # Reserve 10% of data for validation\n",
    "        verbose=1\n",
    "    )\n",
    "    return history\n",
    "\n",
    "def compute_reconstruction_errors(model, X):\n",
    "    \"\"\"\n",
    "    Compute the reconstruction error for each sample.\n",
    "    \n",
    "    Parameters:\n",
    "        model (Model): Trained autoencoder.\n",
    "        X (DataFrame): Input data.\n",
    "        \n",
    "    Returns:\n",
    "        errors (np.array): Reconstruction error (MSE) for each sample.\n",
    "    \"\"\"\n",
    "    # Reconstruct the input using the autoencoder\n",
    "    X_pred = model.predict(X)\n",
    "    # Compute the mean squared error for each sample\n",
    "    errors = np.mean(np.power(X - X_pred, 2), axis=1)\n",
    "    return errors\n",
    "\n",
    "def determine_threshold(errors, quantile=0.95):\n",
    "    \"\"\"\n",
    "    Determine anomaly threshold based on the error distribution.\n",
    "    \n",
    "    Parameters:\n",
    "        errors (np.array): Reconstruction errors.\n",
    "        quantile (float): Quantile value to set threshold (e.g., 95th percentile).\n",
    "        \n",
    "    Returns:\n",
    "        threshold (float): Reconstruction error threshold.\n",
    "    \"\"\"\n",
    "    threshold = np.quantile(errors, quantile)\n",
    "    return threshold\n",
    "\n",
    "def evaluate_anomalies(y_true, errors, threshold):\n",
    "    \"\"\"\n",
    "    Evaluate anomaly detection performance.\n",
    "    \n",
    "    Parameters:\n",
    "        y_true (np.array): Ground truth anomaly labels.\n",
    "        errors (np.array): Reconstruction errors.\n",
    "        threshold (float): Threshold to classify anomalies.\n",
    "        \n",
    "    Returns:\n",
    "        anomaly_labels (np.array): Binary labels (1 for anomaly, 0 for normal).\n",
    "        report (str): Classification report.\n",
    "        auc (float): ROC AUC score.\n",
    "    \"\"\"\n",
    "    # Flag samples with reconstruction error above threshold as anomalies\n",
    "    anomaly_labels = (errors > threshold).astype(int)\n",
    "    \n",
    "    report = classification_report(y_true, anomaly_labels, target_names=['Normal', 'Anomaly'])\n",
    "    auc = roc_auc_score(y_true, errors)\n",
    "    \n",
    "    return anomaly_labels, report, auc\n",
    "\n",
    "# Example usage:\n",
    "# history = train_autoencoder(autoencoder, X.values)\n",
    "# errors = compute_reconstruction_errors(autoencoder, X.values)\n",
    "# threshold = determine_threshold(errors)\n",
    "# anomaly_labels, report, auc = evaluate_anomalies(y, errors, threshold)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# RESULT SAVING & VISUALIZATION\n",
    "# =============================================================================\n",
    "\n",
    "def save_model(model, output_dir, model_filename='autoencoder_model.h5'):\n",
    "    \"\"\"\n",
    "    Save the trained Keras model.\n",
    "    \n",
    "    Parameters:\n",
    "        model (Model): Trained Keras autoencoder.\n",
    "        output_dir (Path): Directory to save the model.\n",
    "        model_filename (str): Name of the model file.\n",
    "    \"\"\"\n",
    "    model_path = output_dir / model_filename\n",
    "    model.save(model_path)\n",
    "    print(\"Model saved to:\", model_path)\n",
    "\n",
    "def save_results(anomaly_labels, errors, threshold, report, auc, output_dir):\n",
    "    \"\"\"\n",
    "    Save evaluation results including anomaly labels, reconstruction errors, threshold, and metrics.\n",
    "    \n",
    "    Parameters:\n",
    "        anomaly_labels (np.array): Predicted anomaly labels.\n",
    "        errors (np.array): Reconstruction errors.\n",
    "        threshold (float): Anomaly threshold.\n",
    "        report (str): Classification report.\n",
    "        auc (float): ROC AUC score.\n",
    "        output_dir (Path): Directory to save the results.\n",
    "    \"\"\"\n",
    "    results_df = pd.DataFrame({\n",
    "        'anomaly_label': anomaly_labels,\n",
    "        'reconstruction_error': errors\n",
    "    })\n",
    "    results_csv = output_dir / 'autoencoder_anomaly_detection_results.csv'\n",
    "    results_df.to_csv(results_csv, index=False)\n",
    "    \n",
    "    # Save threshold and metrics to JSON\n",
    "    metadata = {\n",
    "        'threshold': threshold,\n",
    "        'roc_auc': auc,\n",
    "        'classification_report': report\n",
    "    }\n",
    "    metadata_file = output_dir / 'autoencoder_model_metadata.json'\n",
    "    with open(metadata_file, 'w') as f:\n",
    "        json.dump(metadata, f, indent=2)\n",
    "    \n",
    "    print(\"Results saved to:\", results_csv)\n",
    "    print(\"Metadata saved to:\", metadata_file)\n",
    "\n",
    "def visualize_errors(errors, threshold, output_dir, dpi=300):\n",
    "    \"\"\"\n",
    "    Plot and save a histogram of reconstruction errors with the threshold.\n",
    "    \n",
    "    Parameters:\n",
    "        errors (np.array): Reconstruction errors.\n",
    "        threshold (float): Threshold value.\n",
    "        output_dir (Path): Directory to save the plot.\n",
    "        dpi (int): Resolution for the plot.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.histplot(errors, bins=50, kde=True, color='blue')\n",
    "    plt.axvline(threshold, color='red', linestyle='--', label=f\"Threshold: {threshold:.4f}\")\n",
    "    plt.title('Reconstruction Error Distribution')\n",
    "    plt.xlabel('Reconstruction Error (MSE)')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.legend()\n",
    "    plot_path = output_dir / 'reconstruction_error_distribution.png'\n",
    "    plt.savefig(plot_path, dpi=dpi)\n",
    "    plt.close()\n",
    "    print(\"Error distribution plot saved to:\", plot_path)\n",
    "\n",
    "# Example usage:\n",
    "# save_results(anomaly_labels, errors, threshold, report, auc, OUTPUT_DIR)\n",
    "# save_model(autoencoder, OUTPUT_DIR)\n",
    "# visualize_errors(errors, threshold, OUTPUT_DIR)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# MAIN EXECUTION FUNCTION\n",
    "# =============================================================================\n",
    "\n",
    "def main():\n",
    "    # -----------------------\n",
    "    # Step 1: Load & Preprocess Data\n",
    "    # -----------------------\n",
    "    df = load_data(INPUT_FILE)\n",
    "    X, y, scaler = preprocess_data(df, target_column='faulty')\n",
    "    print(\"Data loaded and preprocessed. Shape of X:\", X.shape)\n",
    "    \n",
    "    # -----------------------\n",
    "    # Step 2: Build & Train the Autoencoder\n",
    "    # -----------------------\n",
    "    input_dim = X.shape[1]\n",
    "    autoencoder = build_autoencoder(input_dim=input_dim, encoding_dim=8)\n",
    "    \n",
    "    # Train the autoencoder (using moderate epochs and batch size for CPU-only training)\n",
    "    history = train_autoencoder(autoencoder, X.values, epochs=50, batch_size=32)\n",
    "    print(\"Autoencoder training complete.\")\n",
    "    \n",
    "    # -----------------------\n",
    "    # Step 3: Evaluate the Model\n",
    "    # -----------------------\n",
    "    errors = compute_reconstruction_errors(autoencoder, X.values)\n",
    "    threshold = determine_threshold(errors, quantile=0.95)\n",
    "    anomaly_labels, report, auc = evaluate_anomalies(y, errors, threshold)\n",
    "    \n",
    "    print(\"Classification Report:\\n\", report)\n",
    "    print(\"ROC AUC Score:\", auc)\n",
    "    print(\"Determined Threshold:\", threshold)\n",
    "    \n",
    "    # -----------------------\n",
    "    # Step 4: Save Results & Model\n",
    "    # -----------------------\n",
    "    save_results(anomaly_labels, errors, threshold, report, auc, OUTPUT_DIR)\n",
    "    save_model(autoencoder, OUTPUT_DIR)\n",
    "    \n",
    "    # -----------------------\n",
    "    # Step 5: Visualize Reconstruction Errors\n",
    "    # -----------------------\n",
    "    visualize_errors(errors, threshold, OUTPUT_DIR)\n",
    "    \n",
    "    # Optionally, save additional artifacts such as the scaler\n",
    "    scaler_file = OUTPUT_DIR / 'scaler.joblib'\n",
    "    joblib.dump(scaler, scaler_file)\n",
    "    print(\"Scaler saved to:\", scaler_file)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
