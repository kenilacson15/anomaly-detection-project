{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Production-Ready One-Class SVM for Anomaly Detection with Visualizations\n",
    "\n",
    "This notebook demonstrates a complete anomaly detection pipeline using One-Class SVM.\n",
    "The pipeline includes:\n",
    "    - Loading and preprocessing a feature-engineered equipment anomaly dataset.\n",
    "    - Training a One-Class SVM model on normal data (non-faulty samples).\n",
    "    - Hyperparameter tuning via grid search with a custom F1-score metric.\n",
    "    - Evaluating the model and generating visualizations such as:\n",
    "         • Confusion matrix heatmap.\n",
    "         • Distribution of decision function scores.\n",
    "         • PCA 2D scatter plot with the decision boundary.\n",
    "    - Saving the model, predictions, and evaluation reports for production deployment.\n",
    "\n",
    "File paths used:\n",
    "    - Dataset: \"C:\\Users\\Ken Ira Talingting\\Desktop\\anomaly-detection-project\\data\\processed\\equipment_anomaly_data_feature_engineered.csv\"\n",
    "    - Output directory: \"C:\\Users\\Ken Ira Talingting\\Desktop\\anomaly-detection-project\\data\\processed_results\"\n",
    "\"\"\"\n",
    "\n",
    "# Import standard libraries for file operations, logging, and time handling\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import joblib  # For saving the model\n",
    "import logging\n",
    "\n",
    "# Import libraries for plotting and visualizations\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Import libraries from scikit-learn for preprocessing, modeling, and evaluation\n",
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score, make_scorer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Configure logging to capture events for production-level monitoring\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define file paths and directories for data input and result output\n",
    "DATA_PATH = r\"C:\\Users\\Ken Ira Talingting\\Desktop\\anomaly-detection-project\\data\\processed\\equipment_anomaly_data_feature_engineered.csv\"\n",
    "RESULTS_DIR = r\"C:\\Users\\Ken Ira Talingting\\Desktop\\anomaly-detection-project\\data\\processed_results\"\n",
    "MODEL_PATH = os.path.join(RESULTS_DIR, \"one_class_svm_model.pkl\")\n",
    "PREDICTIONS_PATH = os.path.join(RESULTS_DIR, \"predictions.csv\")\n",
    "REPORT_PATH = os.path.join(RESULTS_DIR, \"evaluation_report.txt\")\n",
    "CM_PLOT_PATH = os.path.join(RESULTS_DIR, \"confusion_matrix.png\")\n",
    "SCORE_DIST_PLOT_PATH = os.path.join(RESULTS_DIR, \"decision_scores_distribution.png\")\n",
    "PCA_PLOT_PATH = os.path.join(RESULTS_DIR, \"pca_decision_boundary.png\")\n",
    "\n",
    "# Create the results directory if it does not exist to ensure that outputs can be saved\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(filepath: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load dataset from a CSV file located at the given filepath.\n",
    "    \n",
    "    Parameters:\n",
    "        filepath (str): Path to the CSV file.\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: Loaded dataset.\n",
    "    \"\"\"\n",
    "    logging.info(f\"Loading data from {filepath}\")\n",
    "    df = pd.read_csv(filepath)\n",
    "    return df\n",
    "\n",
    "# Example usage:\n",
    "# df = load_data(DATA_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Preprocess the input dataset:\n",
    "      - Converts the 'faulty' column to integer type.\n",
    "      - Separates the dataset into features (X) and labels (y).\n",
    "      - Imputes missing values in features using the mean strategy.\n",
    "      - Scales the features using StandardScaler.\n",
    "      \n",
    "    Note:\n",
    "      - The One-Class SVM is trained exclusively on the normal class (faulty == 0).\n",
    "    \n",
    "    Parameters:\n",
    "        df (pd.DataFrame): Input dataset.\n",
    "        \n",
    "    Returns:\n",
    "        X_scaled (np.ndarray): Scaled feature matrix.\n",
    "        y (pd.Series): Target labels.\n",
    "        scaler (StandardScaler): Fitted scaler (for future transformations).\n",
    "    \"\"\"\n",
    "    # Convert target variable 'faulty' to integer\n",
    "    df['faulty'] = df['faulty'].astype(int)\n",
    "    \n",
    "    # Separate features and target variable\n",
    "    feature_cols = [col for col in df.columns if col != 'faulty']\n",
    "    X = df[feature_cols].copy()\n",
    "    y = df['faulty'].copy()  # 0 for normal, 1 for anomaly\n",
    "\n",
    "    # Impute missing values using the mean of each column\n",
    "    imputer = SimpleImputer(strategy=\"mean\")\n",
    "    X_imputed = imputer.fit_transform(X)\n",
    "    logging.info(\"Missing values have been imputed using the mean strategy.\")\n",
    "\n",
    "    # Scale features to standardize the data distribution\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X_imputed)\n",
    "    logging.info(\"Feature scaling completed using StandardScaler.\")\n",
    "\n",
    "    return X_scaled, y, scaler\n",
    "\n",
    "# Example usage:\n",
    "# X_scaled, y, scaler = preprocess_data(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_training_data(X, y):\n",
    "    \"\"\"\n",
    "    Select the training data for One-Class SVM.\n",
    "    \n",
    "    For One-Class SVM, we train only on normal (non-faulty) samples.\n",
    "    \n",
    "    Parameters:\n",
    "        X (np.ndarray): Feature matrix.\n",
    "        y (pd.Series): Labels (0: normal, 1: anomaly).\n",
    "        \n",
    "    Returns:\n",
    "        X_train (np.ndarray): Subset of X corresponding to normal samples.\n",
    "    \"\"\"\n",
    "    X_train = X[y == 0]\n",
    "    return X_train\n",
    "\n",
    "# Example usage:\n",
    "# X_train = get_training_data(X_scaled, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_score(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Custom scoring function to compute the F1 score.\n",
    "    \n",
    "    The One-Class SVM returns 1 for inliers (normal) and -1 for outliers (anomalies).\n",
    "    This function remaps predictions to match the ground truth:\n",
    "        1 -> 0 (normal)\n",
    "       -1 -> 1 (anomaly)\n",
    "    \n",
    "    Parameters:\n",
    "        y_true (np.ndarray or pd.Series): True labels.\n",
    "        y_pred (np.ndarray): Predictions from One-Class SVM.\n",
    "    \n",
    "    Returns:\n",
    "        float: F1 score.\n",
    "    \"\"\"\n",
    "    # Map One-Class SVM predictions to match our labels\n",
    "    y_pred_mapped = np.where(y_pred == 1, 0, 1)\n",
    "    return f1_score(y_true, y_pred_mapped)\n",
    "\n",
    "# Example usage:\n",
    "# score = custom_score(true_labels, svm_predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_hyperparameters(X_train, X_full, y_full):\n",
    "    \"\"\"\n",
    "    Perform hyperparameter tuning for One-Class SVM using GridSearchCV.\n",
    "    \n",
    "    Even though One-Class SVM is unsupervised, we use known labels for evaluation.\n",
    "    The grid search considers different values for 'nu' and 'gamma'.\n",
    "    \n",
    "    Parameters:\n",
    "        X_train (np.ndarray): Training data (normal samples only).\n",
    "        X_full (np.ndarray): Entire feature matrix (used for evaluation).\n",
    "        y_full (pd.Series): Full set of labels.\n",
    "        \n",
    "    Returns:\n",
    "        best_model (OneClassSVM): Trained model with the best hyperparameters.\n",
    "        best_params (dict): Best hyperparameters found during tuning.\n",
    "    \"\"\"\n",
    "    # Define the hyperparameter grid\n",
    "    param_grid = {\n",
    "        'nu': [0.01, 0.05, 0.1, 0.2],\n",
    "        'gamma': ['scale', 'auto']\n",
    "    }\n",
    "\n",
    "    # Create a wrapper class to make One-Class SVM compatible with GridSearchCV\n",
    "    class OneClassSVMWrapper(OneClassSVM):\n",
    "        def fit(self, X, y=None):\n",
    "            return super().fit(X)\n",
    "        def predict(self, X):\n",
    "            return super().predict(X)\n",
    "\n",
    "    # Use the custom scorer defined earlier\n",
    "    scorer = make_scorer(custom_score, greater_is_better=True)\n",
    "    \n",
    "    # Configure GridSearchCV to tune the One-Class SVM hyperparameters\n",
    "    grid_search = GridSearchCV(estimator=OneClassSVMWrapper(kernel='rbf'),\n",
    "                               param_grid=param_grid,\n",
    "                               scoring=scorer,\n",
    "                               cv=3,\n",
    "                               verbose=1,\n",
    "                               n_jobs=-1)\n",
    "    grid_search.fit(X_train, y=None)\n",
    "    best_params = grid_search.best_params_\n",
    "    logging.info(f\"Best hyperparameters found: {best_params}\")\n",
    "\n",
    "    # Train the final model with the best parameters\n",
    "    best_model = OneClassSVM(kernel='rbf', **best_params)\n",
    "    best_model.fit(X_train)\n",
    "    return best_model, best_params\n",
    "\n",
    "# Example usage:\n",
    "# best_model, best_params = tune_hyperparameters(X_train, X_scaled, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, X, y):\n",
    "    \"\"\"\n",
    "    Evaluate the trained One-Class SVM model using the full dataset.\n",
    "    \n",
    "    The function maps the predictions from {1, -1} to {0 (normal), 1 (anomaly)}\n",
    "    and calculates evaluation metrics such as the confusion matrix and classification report.\n",
    "    \n",
    "    Parameters:\n",
    "        model (OneClassSVM): Trained One-Class SVM model.\n",
    "        X (np.ndarray): Feature matrix for evaluation.\n",
    "        y (pd.Series): True labels.\n",
    "        \n",
    "    Returns:\n",
    "        y_pred_mapped (np.ndarray): Remapped predictions.\n",
    "        cm (np.ndarray): Confusion matrix.\n",
    "        report (str): Detailed classification report.\n",
    "    \"\"\"\n",
    "    # Generate predictions using the model\n",
    "    y_pred = model.predict(X)\n",
    "    \n",
    "    # Map predictions to the expected label format\n",
    "    y_pred_mapped = np.where(y_pred == 1, 0, 1)\n",
    "    \n",
    "    # Compute the confusion matrix and classification report\n",
    "    cm = confusion_matrix(y, y_pred_mapped)\n",
    "    report = classification_report(y, y_pred_mapped)\n",
    "    logging.info(\"Model evaluation completed.\")\n",
    "    return y_pred_mapped, cm, report\n",
    "\n",
    "# Example usage:\n",
    "# y_pred, cm, report = evaluate_model(best_model, X_scaled, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm):\n",
    "    \"\"\"\n",
    "    Plot the confusion matrix as a heatmap and save the figure.\n",
    "    \n",
    "    Parameters:\n",
    "        cm (np.ndarray): Confusion matrix.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.xlabel(\"Predicted Label\")\n",
    "    plt.ylabel(\"Actual Label\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(CM_PLOT_PATH)\n",
    "    plt.close()\n",
    "    logging.info(f\"Confusion matrix plot saved to {CM_PLOT_PATH}\")\n",
    "\n",
    "# Example usage:\n",
    "# plot_confusion_matrix(cm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_decision_score_distribution(model, X, y):\n",
    "    \"\"\"\n",
    "    Plot the distribution of decision function scores for normal and anomalous samples.\n",
    "    \n",
    "    The decision function provides a confidence measure of the predictions.\n",
    "    \n",
    "    Parameters:\n",
    "        model (OneClassSVM): Trained model.\n",
    "        X (np.ndarray): Feature matrix.\n",
    "        y (pd.Series): True labels.\n",
    "    \"\"\"\n",
    "    # Compute decision scores for each sample\n",
    "    scores = model.decision_function(X)\n",
    "    \n",
    "    # Map numeric labels to categorical for visualization\n",
    "    labels = np.where(y == 1, 'Anomaly', 'Normal')\n",
    "    \n",
    "    # Create a DataFrame for plotting\n",
    "    df_scores = pd.DataFrame({\"Score\": scores, \"Label\": labels})\n",
    "    \n",
    "    plt.figure(figsize=(8, 5))\n",
    "    sns.histplot(data=df_scores, x=\"Score\", bins=30, kde=True, hue=\"Label\", palette=\"viridis\")\n",
    "    plt.title(\"Distribution of Decision Function Scores\")\n",
    "    plt.xlabel(\"Decision Score\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(SCORE_DIST_PLOT_PATH)\n",
    "    plt.close()\n",
    "    logging.info(f\"Decision function score distribution plot saved to {SCORE_DIST_PLOT_PATH}\")\n",
    "\n",
    "# Example usage:\n",
    "# plot_decision_score_distribution(best_model, X_scaled, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pca_decision_boundary(model, X, y, scaler):\n",
    "    \"\"\"\n",
    "    Use PCA to project the high-dimensional data into 2D and visualize the decision boundary.\n",
    "    \n",
    "    Parameters:\n",
    "        model (OneClassSVM): Trained model.\n",
    "        X (np.ndarray): Feature matrix.\n",
    "        y (pd.Series): True labels.\n",
    "        scaler (StandardScaler): Fitted scaler (if needed for inverse transformation).\n",
    "    \"\"\"\n",
    "    # Reduce the dimensionality to 2 principal components\n",
    "    pca = PCA(n_components=2)\n",
    "    X_pca = pca.fit_transform(X)\n",
    "    \n",
    "    # Define the mesh grid for plotting the decision boundary\n",
    "    x_min, x_max = X_pca[:, 0].min() - 1, X_pca[:, 0].max() + 1\n",
    "    y_min, y_max = X_pca[:, 1].min() - 1, X_pca[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 200),\n",
    "                         np.linspace(y_min, y_max, 200))\n",
    "    \n",
    "    # Convert grid points back to original feature space using inverse PCA transform\n",
    "    grid = np.c_[xx.ravel(), yy.ravel()]\n",
    "    grid_original = pca.inverse_transform(grid)\n",
    "    \n",
    "    # Evaluate the decision function over the grid\n",
    "    Z = model.decision_function(grid_original)\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    # Plot the decision function using contour plot\n",
    "    plt.contourf(xx, yy, Z, levels=np.linspace(Z.min(), 0, 7), cmap=plt.cm.PuBu)\n",
    "    plt.contour(xx, yy, Z, levels=[0], linewidths=2, colors='darkred')\n",
    "    \n",
    "    # Overlay the original data points in the PCA space\n",
    "    plt.scatter(X_pca[y==0, 0], X_pca[y==0, 1], c='blue', s=20, label='Normal')\n",
    "    plt.scatter(X_pca[y==1, 0], X_pca[y==1, 1], c='orange', s=20, label='Anomaly')\n",
    "    plt.legend()\n",
    "    plt.title(\"PCA Projection with One-Class SVM Decision Boundary\")\n",
    "    plt.xlabel(\"Principal Component 1\")\n",
    "    plt.ylabel(\"Principal Component 2\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(PCA_PLOT_PATH)\n",
    "    plt.close()\n",
    "    logging.info(f\"PCA decision boundary plot saved to {PCA_PLOT_PATH}\")\n",
    "\n",
    "# Example usage:\n",
    "# plot_pca_decision_boundary(best_model, X_scaled, y, scaler)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_results(y_pred, cm, report, scaler, model, best_params):\n",
    "    \"\"\"\n",
    "    Save the trained model, predictions, and evaluation report to disk.\n",
    "    \n",
    "    Parameters:\n",
    "        y_pred (np.ndarray): Final predictions (mapped to {0,1}).\n",
    "        cm (np.ndarray): Confusion matrix.\n",
    "        report (str): Detailed classification report.\n",
    "        scaler (StandardScaler): Fitted scaler for data preprocessing.\n",
    "        model (OneClassSVM): Trained One-Class SVM model.\n",
    "        best_params (dict): Best hyperparameters obtained from grid search.\n",
    "    \"\"\"\n",
    "    # Bundle model components for future production use\n",
    "    model_bundle = {'model': model, 'scaler': scaler, 'best_params': best_params}\n",
    "    joblib.dump(model_bundle, MODEL_PATH)\n",
    "    logging.info(f\"Model bundle saved to {MODEL_PATH}\")\n",
    "\n",
    "    # Save predictions to a CSV file\n",
    "    pred_df = pd.DataFrame({'prediction': y_pred})\n",
    "    pred_df.to_csv(PREDICTIONS_PATH, index=False)\n",
    "    logging.info(f\"Predictions saved to {PREDICTIONS_PATH}\")\n",
    "\n",
    "    # Save the evaluation report (including confusion matrix and classification report) as a text file\n",
    "    with open(REPORT_PATH, 'w') as f:\n",
    "        f.write(\"Evaluation Report\\n\")\n",
    "        f.write(f\"Generated on: {datetime.now()}\\n\\n\")\n",
    "        f.write(\"Confusion Matrix:\\n\")\n",
    "        f.write(np.array2string(cm))\n",
    "        f.write(\"\\n\\nClassification Report:\\n\")\n",
    "        f.write(report)\n",
    "        f.write(\"\\n\\nBest Hyperparameters:\\n\")\n",
    "        f.write(str(best_params))\n",
    "    logging.info(f\"Evaluation report saved to {REPORT_PATH}\")\n",
    "\n",
    "# Example usage:\n",
    "# save_results(y_pred, cm, report, scaler, best_model, best_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to execute the entire One-Class SVM anomaly detection pipeline.\n",
    "    \n",
    "    Steps:\n",
    "      1. Load and preprocess the data.\n",
    "      2. Extract training samples (normal data only) for One-Class SVM.\n",
    "      3. Tune hyperparameters and train the One-Class SVM.\n",
    "      4. Evaluate the model on the full dataset.\n",
    "      5. Save the model, predictions, and evaluation results.\n",
    "      6. Generate and save visualizations.\n",
    "    \"\"\"\n",
    "    # Step 1: Load and preprocess data\n",
    "    df = load_data(DATA_PATH)\n",
    "    X_scaled, y, scaler = preprocess_data(df)\n",
    "    X_train = get_training_data(X_scaled, y)\n",
    "    \n",
    "    # Step 2: Hyperparameter tuning and model training\n",
    "    best_model, best_params = tune_hyperparameters(X_train, X_scaled, y)\n",
    "    \n",
    "    # Step 3: Evaluate the model using the full dataset\n",
    "    y_pred, cm, report = evaluate_model(best_model, X_scaled, y)\n",
    "    \n",
    "    # Step 4: Save the model, predictions, and evaluation report\n",
    "    save_results(y_pred, cm, report, scaler, best_model, best_params)\n",
    "    \n",
    "    # Step 5: Generate and save visualizations for performance insights\n",
    "    plot_confusion_matrix(cm)\n",
    "    plot_decision_score_distribution(best_model, X_scaled, y)\n",
    "    plot_pca_decision_boundary(best_model, X_scaled, y, scaler)\n",
    "    \n",
    "    logging.info(\"One-Class SVM pipeline executed successfully.\")\n",
    "\n",
    "# Run the pipeline when this cell is executed as a script\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
